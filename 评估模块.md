# CoT 思维链自动化生成与评估流水线

本项目实现了一条 **PDF → 文本 → SFT 数据 → CoT 评估 → 数据持久化** 的 LangGraph 流水线。输出 JSON 可直接用于大模型监督微调（SFT）。  

## 新增模块概览

- **graph/**：在原有基础上新增子目录 `evaluation/cot_evaluator.py`，用于 **思维链（CoT）质量评估**。主图 `pipeline.py` 负责工作流编排，新增 1. **run_evaluation**： 
异步调用评估子图，输入 `case_record`，输出 `evaluation_results`（含整体评分 `overall_score` 和各维度评分）。2.**check_evaluation_score**：根据 `overall_score` 决定流程：`>=0.9` → 保存案例（persist_dataset）`<0.9` → 重新生成案例 ；
- **prompts/**：所有提示词通过 **ai-prompter + Jinja** 管理。


> 新增内容重点在 **CoT 评估部分**，以下详细说明。
## CoT 质量评估（graph/evaluation/cot_evaluator.py）

该子图负责对生成的CoT进行全面质量评估，覆盖工具使用、流程合理性及推理正确性等维度，确保输出数据可用于监督训练或后续分析。

### 主要功能

1. **工具可执行性评估（规则）**
   - 检查每个工具调用是否在工具库中定义，参数是否完整及类型正确。
   - 输出 `tool_executability_results` 与分数 `tool_executability_score`。

2. **工具工作流合理性评估（规则）**
   - 分析工具调用顺序是否符合预期流程（如 query → analyze → investigate → action）。
   - 输出 `tool_workflow_results`，标记重复调用或顺序异常。

3. **工具使用必要性评估（LLM）**
   - 对每次工具调用是否必要进行评估，使用 LLM 判断是否可省略或替代。
   - 输出 `tool_necessity_results` 与整体 `tool_necessity_score`。

4. **工具因果贡献评估（LLM）**
   - 判断每个工具调用对最终结论的贡献及重要性。
   - 输出 `tool_causality_results` 与分数 `tool_causality_score`。

5. **工具链因果连贯性评估（LLM）**
   - 分析连续工具调用的因果关系，确保前一工具输出驱动下一工具使用。
   - 输出 `tool_causal_chain_results` 与分数 `tool_causal_chain_score`。

6. **推理正确性评估（LLM）**
   - 对整个案例的推理过程进行正确性检查，包括逻辑错误、证据质量、推理缺口。
   - 输出 `correctness_results` 与分数 `correctness_score`。

7. **综合评分与质量等级**
   - 将各模块分数加权汇总，生成总分 `overall_score`。
   - 根据总分划分质量等级（A+ 到 F）。
   - 同时生成详细问题报告（issues），标注工具问题、工作流异常、必要性低、因果不连贯等。

> 该评估模块支持异步调用，能够处理复杂工具调用轨迹，并结合规则与 LLM 判断，实现对 CoT 案例质量的精细化评价。

