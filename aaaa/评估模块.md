#  Evaluation Pipeline

提供了一套 **基于 LangGraph 的 LLM 推理过程（CoT / Tool-use）质量评估 Pipeline**，用于对已生成的 `case_record` 进行**自动化、多维度评估**，并将评估结果持久化存储。



---

##  功能概览

### 1. 输入

- **已生成的 `case_record`**（JSON / JSONL）
- **可选缓存文本 `excerpt`**

### 2. 评估内容（由 `cot_evaluator.py` 完成）
管道评估输出的指标如下（0~1，越高越好）：

| 指标 | 描述                  |
|------|---------------------|
| `TOOL_EXECUTABILITY` | 工具调用是否可执行，参数是否完整正确  |
| `TOOL_WORKFLOW` | 工具调用流程是否合理，是否符合预期顺序 |
| `TOOL_NECESSITY` | 工具调用是否必要，是否冗余       |
| `TOOL_CAUSALITY` | 单个工具调用对最终结果的因果贡献    |
| `TOOL_CAUSAL_CHAIN` | 工具前后调用是否具有因果关系      |
| `CORRECTNESS` | 推理逻辑是否正确，结论是否合理     |
| `CONSISTENCY` | 推理内容与原始文档/事实的一致性    |
| `SUPPLEMENTARY_RATIONALITY` | 补充内容是否合理、必要，是否无过度臆造 |

### 3. 输出

- 标准化评估结果（JSONL）
- 每个 `document_id` 对应一个评估文件，保存至 `dataset_store_eval` 目录

---

##  项目结构

```text
data_pipeline_eval/
├── graph/
│   ├── pipeline_eval.py          # 评估主 pipeline
│   └── evaluation/
│       └── cot_evaluator.py      # 子评估图（evaluation_graph）
├── store/
│   └── dataset_store.py          # JSONL 持久化存储
├── utils/
│   └── settings.py               # PipelineSettings 定义
├── data/
│   ├── datasets/                 # case_record 存储目录
│   └── datasets_evaluation/      # evaluation 结果输出目录
└── README.md
```

